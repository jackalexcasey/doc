/*
 * Copyright (C) 2010 Cisco Systems
 * Author: Etienne Martineau <etmartin@cisco.com>
 *
 * This work is licensed under the terms of the GNU GPL, version 2.
 */

#include "config.h"

static struct timespec carrier_ts = {
	.tv_sec = 0,
	/* Here we provision for the jitter on the timer */
	.tv_nsec = FRAME_PERIOD_IN_NSEC - TIMER_JITTER_IN_NSEC,
};

static void calibrated_ldelay(cycles_t loops)
{
	cycles_t t1;
	t1 = get_cycles();
	while(get_cycles() - t1 < loops );
}

#if 0
/*
 * Suppose 8Kb L1 cache ,4way ,64byte per line
 * ==> 8kb / 64byte == 128 lines
 * ==> 128 lines / 4way == 32 sets
 * ==> 32 sets * 64 byte = 2048 wrap value
 */
unsigned char l1_8kdata[1024*8];
void clear_8k_l1_cache_set()
{
	int set;
	for(set=0;set<31;set++){
		l1_8kdata[(64*set) + 0] = 0xff;
		l1_8kdata[(64*set) + 2048] = 0xff;
		l1_8kdata[(64*set) + 4096] = 0xff;
		l1_8kdata[(64*set) + 6144] = 0xff;
	}
}

/*
 * Suppose 32Kb L1 cache ,8way ,64byte per line
 * ==> 32kb / 64byte == 512 lines
 * ==> 512 lines / 8way == 64 sets
 * ==> 64 sets * 64 byte = 4096 wrap value
 */
volatile unsigned char dummy;
volatile unsigned char l1_wrapdata[1024*256];

# define __force    __attribute__((force))
static inline void clflush(volatile void *__p)
{
	asm volatile("clflush %0" : "+m" (*(volatile char __force *)__p));
}

void zap_line_in_cache_set(int setnr)
{
	int set;

	set = 64*(setnr);
//	dummy = l1_wrapdata[set+0];
//__builtin_prefetch(&l1_wrapdata[set+0]);
	clflush(&l1_wrapdata[set+0]);

}
#define barrier() asm volatile("" ::: "memory")
volatile unsigned char l1_32kdata[1024*32];
cycles_t measure_cache_set_access_time(int setnr)
{
	int set;
	cycles_t t1;

	set = 64*(setnr);
	t1 = get_cycles();
	__asm__ __volatile__("mfence" ::: "memory");
	__builtin_prefetch(l1_32kdata[set + 0]);
	__builtin_prefetch(l1_32kdata[set + 4096]);
	__builtin_prefetch(l1_32kdata[set + 8192]);
	__builtin_prefetch(l1_32kdata[set + 12288]);
	__builtin_prefetch(l1_32kdata[set + 16384]);
	__builtin_prefetch(l1_32kdata[set + 20480]);
	__builtin_prefetch(l1_32kdata[set + 24576]);
	__builtin_prefetch(l1_32kdata[set + 28672]);
	return get_cycles() - t1;
}

void pll(void(*fn)(cycles_t))
{
	int x;
	cycles_t array[100];

	zap_line_in_cache_set(0);
	array[0] = measure_cache_set_access_time(0);

	for(x=0;x<100;x++){
		zap_line_in_cache_set(25);
		array[x] = measure_cache_set_access_time(x);
	}

	for(x=0;x<100;x++){
		fprintf(stderr,"_%Ld_",array[x]);
	}
}
#endif

volatile unsigned char l1_32kdata[1024*32];

/*
Invalidates the cache line that contains the linear address specified with the source operand from all levels of the processor cache hierarchy (data and instruction). The invalidation is broadcast throughout the cache coherence domain. If, at any level of the cache hierarchy, the line is inconsistent with memory (dirty) it is written to memory before invalidation. The source operand is a byte memory location.
The availability of CLFLUSH is indicated by the presence of the CPUID feature flag CLFSH (bit 19 of the EDX register, see Section , CPUID-CPU Identification). The aligned cache line size affected is also indicated with the CPUID instruction (bits 8 through 15 of the EBX register when the initial value in the EAX register is 1).
The memory attribute of the page containing the affected line has no effect on the behavior of this instruction. It should be noted that processors are free to speculatively fetch and cache data from system memory regions assigned a memory-type allowing for speculative reads (such as, the WB, WC, and WT memory types). PREFETCHh instructions can be used to provide the processor with hints for this speculative behavior. Because this speculative fetching can occur at any time and is not tied to instruction execution, the CLFLUSH instruction is not ordered with respect to PREFETCHh instructions or any of the speculative fetching mechanisms (that is, data can be speculatively loaded into a cache line just before, during, or after the execution of a CLFLUSH instruction that references the cache line).
CLFLUSH is only ordered by the MFENCE instruction. It is not guaranteed to be ordered by any other fencing or serializing instructions or by another CLFLUSH instruction. For example, software can use an MFENCE instruction to insure that previous stores are included in the writeback.
The CLFLUSH instruction can be used at all privilege levels and is subject to all permission checking and faults associated with a byte load (and in addition, a CLFLUSH instruction is allowed to flush a linear address in an execute-only segment). Like a load, the CLFLUSH instruction sets the A bit but not the D bit in the page tables.
The CLFLUSH instruction was introduced with the SSE2 extensions; however, because it has its own CPUID feature flag, it can be implemented in IA-32 processors that do not include the SSE2 extensions. Also, detecting the presence of the SSE2 extensions with the CPUID instruction does not guarantee that the CLFLUSH instruction is implemented in the processor.
*/
# define __force    __attribute__((force))
static inline void clflush(volatile void *__p)
{
	asm volatile("clflush %0" : "+m" (*(volatile char __force *)__p));
}

static inline void prefetch(volatile void *__p)
{
	asm volatile("prefetcht0 %0" : "+m" (*(volatile char __force *)__p));
}
#define mb()    asm volatile("mfence":::"memory")

static inline void native_cpuid(unsigned int *eax, unsigned int *ebx,
                unsigned int *ecx, unsigned int *edx)
{
    /* ecx is often an input as well as an output. */
    asm volatile("cpuid"
        : "=a" (*eax),
          "=b" (*ebx),
          "=c" (*ecx),
          "=d" (*edx)
        : "0" (*eax), "2" (*ecx)
        : "memory");
}


void *ptr = NULL;

void open_c(void)
{
	int fd;


	if ((fd = shm_open("channel", O_CREAT|O_RDWR,
					S_IRWXU|S_IRWXG|S_IRWXO)) > 0) {
		if (ftruncate(fd, 1024*128) != 0)
			DIE("could not truncate shared file\n");
	}
	else
		DIE("Open channel");
	
	ptr = mmap(NULL,1024*128,PROT_READ|PROT_WRITE,MAP_SHARED,fd,0);
	if(ptr == MAP_FAILED)
		DIE("mmap");
	fprintf(stderr, "mmap ptr %p\n",ptr);
}

void zap_line_in_cache_set(int setnr)
{
//__builtin_prefetch(&l1_wrapdata[set+0]);
//	clflush(&l1_32kdata[setnr]);
	clflush(&ptr[setnr]);
}
volatile unsigned char dummy;
cycles_t measure_cache_set_access_time(int setnr)
{
	int set;
	cycles_t t1;
	unsigned int a,b,c,d;

	t1 = get_cycles();
//	mb();
	//prefetch(&l1_32kdata[setnr]);
//	__builtin_prefetch(&l1_32kdata[setnr]);
	mb();
	dummy = ((unsigned char*)ptr)[setnr];
//	mb();
//	__builtin_prefetch(&ptr[setnr]);
//	native_cpuid(&a,&b,&c,&d);
//	__asm__ __volatile__("mfence" ::: "memory");
	return get_cycles() - t1;
}

void pll(void(*fn)(cycles_t))
{
	int x;
	cycles_t array[100];

	open_c();

	zap_line_in_cache_set(0);
	array[0] = measure_cache_set_access_time(0);

	for(x=0;x<100;x++){
		zap_line_in_cache_set(10);
		array[x] = measure_cache_set_access_time(1024);
	}

	for(x=0;x<100;x++){
		fprintf(stderr,"_%Ld_",array[x]);
	}
}

#if 0
void pll(void(*fn)(cycles_t))
{
	int x;
	cycles_t t1, t2, t3, phase, delta, lpj;
	
restart:
	/*
	 * Here we adjust the phase on an integer multiple of a frame
	 * TODO relax CPU here
	 */
	while(  ((t2 = get_cycles()) &~0xff) % ((FRAME_PERIOD_IN_CYCLE*FRAME_FREQ) &~0xff) );
	fprintf(stderr, "%Ld %Ld %Ld\n",FRAME_PERIOD_IN_CYCLE, FRAME_PERIOD_IN_NSEC, t2);

	phase = 0;
	x=0;

	while(1){
		t1 = get_cycles();

		if(clock_nanosleep(CLOCK_MONOTONIC, TIMER_RELTIME, &carrier_ts, NULL))
			DIE("clock_nanosleep");
		
		/*
		 * Here 'delta' correspond to the amount of cycle taken away by 
		 * nanosleep(). It cannot exceed the FRAME_PERIOD
		 */
		delta = (get_cycles() - t1)/2;
		if(delta > FRAME_PERIOD_IN_CYCLE){
			fprintf(stderr, "CLOCK Synchronization lost! %Lu %Lu\n",FRAME_PERIOD_IN_CYCLE, delta);
			goto restart;
		}

		/*
		 * Then we do lpj compensation
		 */
		lpj = (FRAME_PERIOD_IN_CYCLE - delta - phase -((t1-t2)/2) + PHASE_OFFSET/2 );
		if(lpj < 0){
			fprintf(stderr, ".");
			lpj = 0;
		}
		calibrated_ldelay(lpj*2);

		/*
		 * Phase compensation:
		 *
		 * At t2 we are monotonic but we can be out of phase.
		 *
		 * The (t1-t2) compensation take into account the overall loop 
		 * execution time but even then phase lag can accumulate.
		 *   Example is preemption right after calibrated_ldelay _but_ 
		 *   before t2 = get_cycles();
		 *
		 * In general phase lag will accumulate over time ( i.e we integrate 
		 * the noise ) but it is generally constant for each iteration since
		 * we are dealing with white noise
		 */
		t2 = get_cycles();

		fn(t2);

		phase = ((FRAME_PERIOD_IN_CYCLE/2) - 
			abs( (t2 % FRAME_PERIOD_IN_CYCLE)/2 - FRAME_PERIOD_IN_CYCLE/2) );

		/*
		 * t3 defines the amount of cycle taken by modulation
		 */
		t3 = (get_cycles() - t2)/2;
		if(t3 > PAYLOAD_AVAILABLE_CYCLE){
			fprintf(stderr, "PAYLOAD Synchronization lost! %Lu %Lu\n",PAYLOAD_AVAILABLE_CYCLE, t3);
			goto restart;
		}

		if(x && !(x%60))
			fprintf(stderr, "%Ld %Ld/%Ld %Ld\n", t2, t3,PAYLOAD_AVAILABLE_CYCLE, TIMER_JITTER_IN_CYCLE);

		x++;
	}
}
#endif
